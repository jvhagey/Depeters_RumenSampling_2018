---
title: "Depeters Study 2018"
author: "Jill Hagey"
date: "August 7, 2018"
output: 
  html_document:
    theme: spacelab 
    toc: true
    toc_depth: 2
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loading packages, warning=FALSE}
#load the packages
#library(dada2); packageVersion("dada2")
library(phyloseq); packageVersion("phyloseq")
library(breakaway); packageVersion("breakaway")
library(DivNet); packageVersion("DivNet")
#library(corncob); packageVersion("corncob")
library(ggplot2); packageVersion("ggplot2")
library(reshape2); packageVersion("reshape2")
library(DESeq2); packageVersion("DESeq2")
library(plotly); packageVersion("plotly")
library(dplyr); packageVersion("dplyr")
```

```{r setting working directory, include=FALSE}
setwd("C:/Users/jvhagey/OneDrive - UC Davis/Documents/collaboration/Depeters")
#setting working directory
setwd("C:/Users/Jill/OneDrive - UC Davis/Documents/collaboration/Depeters")
```
#Running DADA2 to get ASVs and assign taxonomy

Lots of code involved here, but I'll spare you the details.

```{r Running  DESEq2, eval=FALSE, include=FALSE}
# CHANGE ME to the directory containing the fastq files after unzipping.
path <- "C:/Users/jvhagey/Desktop/Depeters/" 
list.files(path)
# Forward and reverse fastq filenames have format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq
fnFs <- sort(list.files(path, pattern="_Trim_R1.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_Trim_R2.fastq.gz", full.names = TRUE))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
plotQualityProfile(fnFs[1:10])
plotQualityProfile(fnRs[1:10])

#Place filtered files in filtered/subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))

out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(240,220),trimLeft=c(10,0),
                     maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE, minLen=150,
                     compress=TRUE, multithread=FALSE, verbose=TRUE) 
head(out)

#check quality again after trimming
plotQualityProfile(filtFs[10:20])
plotQualityProfile(filtRs[10:20])

#learn erros for DADA2 algorithm
errF <- learnErrors(filtFs, multithread=FALSE)
errR <- learnErrors(filtRs, multithread=FALSE)

saveRDS(errF, "C:/Users/jvhagey/Desktop/Depeters/errF.rds")
saveRDS(errR, "C:/Users/jvhagey/Desktop/Depeters/errR.rds")

plotErrors(errF, nominalQ=TRUE)

derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
saveRDS(derepFs, "C:/Users/jvhagey/Desktop/Depeters/derepFs.rds")
saveRDS(derepRs, "C:/Users/jvhagey/Desktop/Depeters/derepRs.rds")

# Name the derep-class objects by the sample names
names(derepFs) <- sample.names
names(derepRs) <- sample.names
#run the dada2 algorithum
dadaFs <- dada(derepFs, err=errF, multithread=FALSE, pool=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=FALSE, pool=TRUE)
#checking output
dadaFs[[1]]

#Merging forward and Reverse Reads
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])

#Construct Sequence Table
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
#Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
#Removing chimeras
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=FALSE, verbose=TRUE)
dim(seqtab.nochim)
#checking Frequencing of chimeras
sum(seqtab.nochim)/sum(seqtab)
#Tracking read count through pipeline
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
track

taxa_rdp <- assignTaxonomy(seqtab.nochim, "/share/tearlab/Maga/Jill/rdp_train_set_16.fa.gz", multithread=TRUE)
saveRDS(taxa_rdp, "/share/tearlab/Maga/Jill/16s_Milk_2016/DADA2/taxa_rdp.rds")
taxa.sp_rdp <- addSpecies(taxa_rdp, "/share/tearlab/Maga/Jill/rdp_species_assignment_16.fa.gz")
saveRDS(taxa.sp_rdp, "/share/tearlab/Maga/Jill/16s_Milk_2016/DADA2/taxa.sp_rdp.rds")
#
taxa_silva <- assignTaxonomy(seqtab.nochim, "/share/tearlab/Maga/Jill/silva_nr_v132_train_set.fa.gz", multithread=TRUE)
saveRDS(taxa_silva, "/share/tearlab/Maga/Jill/16s_Milk_2016/DADA2/taxa_silva.rds")
taxa.sp_silva <- addSpecies(taxa_silva, "/share/tearlab/Maga/Jill/silva_species_assignment_v132.fa.gz")
saveRDS(taxa.sp_silva, "/share/tearlab/Maga/Jill/16s_Milk_2016/DADA2/taxa.sp_silva.rds")

taxa.print <- taxa_rdp # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)

#getting sequences from object to make tree
otu <- seqtab.nochim
asv_seqs<-rownames(otu)
## Formatting for fastas
fasta_format<-paste0(">",asv_seqs,"\n",asv_seqs)
## Writing output file
write(fasta_format,"asv_seqs.fa")
#This was then imported was put into qiime1 to make tree with FastTree

```

#Making phyloseq object
```{r Making phyloseq object}
setwd("C:/Users/Jill/OneDrive - UC Davis/Documents/collaboration/Depeters/DADA2_Out")
seqtab.nochim <- readRDS("seqtab.nochim.rds")
#taxa_sp <- readRDS("taxa.sp_rdp.rds") #gave 8,997 Unassigned ASVs
taxa_sp <- readRDS("taxa.sp_silva.rds") #gave 8,549 Unassigned ASVs 
#had the following taxa that rdp didn't Entotheonellaeota, Epsilonbacteraeota, Gemmatimonadetes, Kiritimatiellaeota, Patescibacteria, BRC1
#it doesn't have SR1 or Candidatus_Saccharibacteria though
TREE <- read_tree("dada_seqs2.tre")
MAP <- import_qiime_sample_data("C:/Users/jvhagey/OneDrive - UC Davis/Documents/collaboration/Depeters/Mapping_File_MMDR.txt")
ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), sample_data(MAP), tax_table(taxa_sp), phy_tree(TREE))
ps
#Not sure why, but the ASV sequences are columns not rows like they normally are so transposing the otu_table
otu_table(ps) <- t(otu_table(ps))
```

#Control Samples

```{r}
#Checking the taxa present in kits
ps_FecalKit <- subset_samples(ps, Sample_Type == c("TMR_fecal_kit"))
ps_PlantKit <- subset_samples(ps, Sample_Type == c("TMR_plant_kit"))


#removing control kits from overall samples
ps <- subset_samples(ps, Sample_Type != c("TMR_fecal_kit"))
ps <- subset_samples(ps, Sample_Type != c("TMR_plant_kit"))
```

#Cleaning data
Currently, we are starting with 23,727 ASVs from 68 samples

```{r}
#removing control kits from dataset
ps <- subset_samples(ps, Sample_Type != c("TMR_fecal_kit"))
ps <- subset_samples(ps, Sample_Type != c("TMR_plant_kit"))
#Checking for empty samples, samples with no taxa assoicated with them (should be "FALSE").
any(sample_sums(ps) == 0)
#Checking if there are ASVs that aren't present in any samples (should be "FALSE")
any(taxa_sums(ps) == 0)
#Determining how many ASVs there are that aren't present in any sample
sum(taxa_sums(ps) == 0)
#removing ASVs that aren't present in any samples
ps <- prune_taxa(taxa_sums(ps) > 0, ps)
ps
```

There was no empty samples or taxa which is what we want.

#Looking at metrics after filtering

```{r}
#number of taxa present
ntaxa(ps)
#checking names of taxa present at specific rank
get_taxa_unique(ps, "Phylum")
get_taxa_unique(ps, "Order")
get_taxa_unique(ps, "Family")
get_taxa_unique(ps, "Genus")
```


```{r}
#How many singletons are there? How many doubletons? before and after filtering
singletons <- sum(rowSums(ps@otu_table@.Data)==1) #number of singletons with controls and OTUs not present in any sample removed
doubletons <- sum(rowSums(ps@otu_table@.Data)==2) #number of doubletons with controls and OTUs not present in any sample removed
tripletons <- sum(rowSums(ps@otu_table@.Data)==3)
sum(singletons,doubletons,tripletons)
```

Looks like there is no singletons, doubletons, and tripletons which is a bit atypical. This could be due to DADA2 pipeline and removing superfiluous reads (i.e. chimeras and read error). Its also probably a product of having 68 samples and getting deep sequencing. 

```{r}
#Checking which samples have fewer than 5000 reads
#function returns sample name and its column number
which(!rowSums(otu_table(ps)) > 5000)
```

#More cleaning of data

```{r}
#Create table, number of features for each phyla
table(tax_table(ps)[, "Phylum"], exclude = NULL)
#Removing ambiguous phylum annotation
#This changes ASVs from 23,727 to 15,178
ps <- subset_taxa(ps, !is.na(Phylum) & !Phylum %in% c("", "uncharacterized"))
ps
```

There was 8,549 AVSs that weren't assigned to a phyla so these were be removed for analysis. This leaves 15,178 ASVs.

```{r}
#removing phyla that are assigned to archea and Eukaryotes as these are not suppose to be amplified by the primer pair
#Define phyla to filter
filterPhyla = c("Euryarchaeota","Euglenozoa")
ps <- subset_taxa(ps, !Phylum %in% filterPhyla)
ps

#Compute prevalence of each feature, store as data.frame
#prevalence in the dataset we will define here as the number of samples in which a taxon appears at least once
prevdf = apply(X = otu_table(ps),
               MARGIN = ifelse(taxa_are_rows(ps), yes = 1, no = 2),
               FUN = function(x){sum(x > 0)})
#Add taxonomy and total read counts to this data.frame
prevdf = data.frame(Prevalence = prevdf, TotalAbundance = taxa_sums(ps), tax_table(ps))
#display table
plyr::ddply(prevdf, "Phylum", function(df1){cbind(mean(df1$Prevalence),sum(df1$Prevalence))})
```

After filtering out Eukaryotes we have 15,058 ASVs left.

Here we see thht Acidobacteria only has one feature so just looking into this real quick.

```{r}
#Making phyloseq object with Acidobacteria
ps_explore <- subset_taxa(ps, Phylum == c("Acidobacteria"))
ps_explore@sam_data$Sample_Type
```

The phyla Acidobacteria is only in solid samples, interesting.

```{r include=FALSE}
#Saving a copy of the ASV sequences for later
ps_copy <- ps
#Changing the name of the ASVs to numbers to make tables easier to read
#I mostly needed to do this for specifing the base taxa in divnet function easier.
taxa_names(ps) <- paste0("Seq", seq(ntaxa(ps)))
```

```{r, fig.cap = "..."}
#Subset to the remaining phyla
prevdf1 = subset(prevdf, Phylum %in% get_taxa_unique(ps, "Phylum"))
ggplot(prevdf1, aes(TotalAbundance, Prevalence / nsamples(ps),color=Phylum)) +
#Include a guess for parameter
  geom_hline(yintercept = 0.05, alpha = 0.5, linetype = 2) +  geom_point(size = 2, alpha = 0.7) +
  scale_x_log10() +  xlab("Total Abundance") + ylab("Prevalence [Frac. Samples]") +
  facet_wrap(~Phylum) + theme(legend.position="none")
```

#Examining data for unsupervised exploratory analysis

```{r}
qplot(log10(rowSums(otu_table(ps))),binwidth=0.2) +
  xlab("Logged counts-per-sample")
```

Going to use log transformations for normalizing for library size during exploratory analysis. As this looks appropraite for the "tailed" data.

```{r Bray-Curtis distance}
set.seed(1850)
#bray curtis distance
pslog <- transform_sample_counts(ps, function(x) log(1 + x))
out.pcoa.log <- ordinate(pslog,  method = "MDS", distance = "bray")
evals <- out.pcoa.log$values[,1]
plot_ordination(pslog, out.pcoa.log, color = "Sample_Type") +
  labs(col = "Sample Type")+
  coord_fixed(sqrt(evals[2] / evals[1]))
```

Looks like most fecal samples pull away from the other samples on the first axis.
Solid and Liquid strained samples move higher on the 2nd axis, but this difference is half that of the differences between fecal samples and all other samples. Overall, it appears that there is 3 "clusters".

```{r}
set.seed(1850)
#weighted unifrac
out.wuf.log <- ordinate(pslog, method = "MDS", distance = "wunifrac")
evals <- out.wuf.log$values$Eigenvalues
eval_per_wuf <- (out.wuf.log$values$Eigenvalues/(sum(out.wuf.log$values$Eigenvalues)))*100
#Plotting eigenvalues to determine how many axis should be shown in graph
barplot(eval_per_wuf[1:10],names.arg=paste0('Eigenvalue',1:10), ylab="Percent of explained variances", col="blue")
```

From the eigenvalues we can see that 2 axis is appropriate for graphing, together explaining almost 70% of the variance between the samples. 

```{r}
plot_ordination(pslog, out.wuf.log, color = "Sample_Type") +
  labs(col = "Sample Type", title="Weighted Unifrac") +
  coord_fixed(sqrt(evals[2] / evals[1]))
#ploting same weighted unifrac with phyla colored
plot_ordination(pslog, out.wuf.log, type = "species", color = "Phylum") +
  coord_fixed(sqrt(evals[2] / evals[1]))
```

Now that we take into account phylogenetic information in the distance metric we see a similar clustering pattern as with the bray-curtis. However, now samples from feces, liquid strained and solid samples track similarly on the 1st axis (which explains 55.8% of the variation). Feces still is separated from the other samples on the 2nd axis. Also, although not quite as clean there still seems like there is 3 "clusters"

Next I will look futher into the phyla that are driving this differences. As firmicutes, not surprisingly, are covering the entire graph so we remove the Firmicutes and have a second look at this. 

```{r}
#Removing Firmicutes to visulaize other phyla
ps_NoFirm = subset_taxa(pslog, Phylum != c("Firmicutes"))
pslog_NoFirm <- transform_sample_counts(ps_NoFirm, function(x) log(1 + x))
out.wuf.log.NoFirm <- ordinate(pslog_NoFirm, method = "MDS", distance = "wunifrac")
evals_NoFirm <- out.wuf.log.NoFirm$values$Eigenvalues
#ploting same weighted unifrac with phyla colored
plot_ordination(pslog_NoFirm, out.wuf.log.NoFirm, type = "species", color = "Phylum") +
  coord_fixed(sqrt(evals_NoFirm[2] / evals_NoFirm[1]))

#Removing Bacteroidetes to visulaize other phyla
ps_NoMain = subset_taxa(ps_NoFirm, Phylum != c("Bacteroidetes"))
pslog_NoMain <- transform_sample_counts(ps_NoMain, function(x) log(1 + x))
out.wuf.log.NoMain <- ordinate(pslog_NoMain, method = "MDS", distance = "wunifrac")
evals_NoMain <- out.wuf.log.NoMain$values$Eigenvalues
#ploting same weighted unifrac with phyla colored
plot_ordination(pslog_NoMain, out.wuf.log.NoMain, type = "species", color = "Phylum") +
  coord_fixed(sqrt(evals_NoMain[2] / evals_NoMain[1]))
```

Now that the Firmicutes are removed from the graph the data suggest that most of the varitation (1st axsis) comes from Bacteroidetes ASVs are primarily driving the distances on the 1st axis. Thus, the data suggests that fecal, soild and some liquid samples have much less ASVs assigned to Bacteroidetes than other samples.

The second axis in part is driven by Acidobacteria (samples lower on the 2nd axis) and Proteobacteria/Kiritimatiellaeota or Gemmatimonadetes? - I can't tell which - (smaples higher on the 2nd axis).

We will come back to examining these differences further during differential abundance testing.

For good measure we will look at the unweighted unifrac that puts more weight on rare species as well.

```{r}
#UnWeighted unifrac
out.unwuf.log <- ordinate(pslog, method = "MDS", distance = "unifrac")
evals_un <- out.unwuf.log$values$Eigenvalues
plot_ordination(pslog, out.unwuf.log, color = "Sample_Type") +
  labs(col = "Sample Type", title="Unweighted unifrac") +
  coord_fixed(sqrt(evals_un[2] / evals_un[1]))
eval_per_unwuf <- (out.unwuf.log$values$Eigenvalues/(sum(out.unwuf.log$values$Eigenvalues)))*100
#Plotting eigenvalues to determine how many axis should be shown in graph
barplot(eval_per_unwuf[1:10],names.arg=paste0('Eigenvalue',1:10), ylab="Percent of explained variances", col="blue")
```

Notice that less of the variation is explained in each axis with the unweighted versus the weighted unifrac. The distances between points aren't as great with the unweighted versus weighted unifrac, thus it seems that the difference between sample types is due to abundance differences and less about differences in species.

The eigenvalues here suggest a 3rd axis would help capture more of the variation.

```{r 3D plotting, warning=FALSE}
#Making 3D plot with plotly
plot_ly(data.frame(out.unwuf.log$vectors), x=~Axis.1, y=~Axis.2, z=~Axis.3, color=sample_data(ps)$Sample_Type) %>%   layout(xaxis=list(title="PC1 25.3%"),
         yaxis=list(title="PC2 12.7%"),
         zaxis=list(title="PC3 8.4%"))
```

```{r}
#ploting same weighted unifrac with phyla colored
plot_ordination(pslog, out.unwuf.log, type = "species", color = "Phylum") +
  coord_fixed(sqrt(evals_un[2] / evals_un[1]))

#Removing Firmicutes to visulaize other phyla
out.unwuf.log.NoFirm <- ordinate(pslog_NoFirm, method = "MDS", distance = "unifrac")
evals_unw_NoFirm <- out.unwuf.log.NoFirm$values$Eigenvalues
#ploting same weighted unifrac with phyla colored
plot_ordination(pslog_NoFirm, out.unwuf.log.NoFirm, type = "species", color = "Phylum") +
  coord_fixed(sqrt(evals_unw_NoFirm[2] / evals_unw_NoFirm[1]))

#Removing Bacteroidetes to visulaize other phyla
out.unwuf.log.NoMain <- ordinate(pslog_NoMain, method = "MDS", distance = "unifrac")
evals_unw_NoMain <- out.unwuf.log.NoMain$values$Eigenvalues
#ploting same weighted unifrac with phyla colored
plot_ordination(pslog_NoMain, out.unwuf.log.NoMain, type = "species", color = "Phylum") +
  coord_fixed(sqrt(evals_unw_NoMain[2] / evals_unw_NoMain[1]))
```

The distances between points aren't as great with the unweighted versus weighted unifrac, thus it seems that the difference between sample types is due to abundance differences and less about differences in species.

#Differential Abundance Testing with DESeq2/CornCob

To speed this up initial abundance testing filtering out lower abundance taxa first

```{r}
#Compute prevalence of each feature, store as data.frame
prevdf = apply(X = otu_table(ps), MARGIN = ifelse(taxa_are_rows(ps), yes = 1, no = 2),
               FUN = function(x){sum(x > 0)})
#Add taxonomy and total read counts to this data.frame
prevdf = data.frame(Prevalence = prevdf, TotalAbundance = taxa_sums(ps),tax_table(ps))

#Define prevalence threshold as 5% of total samples
prevalenceThreshold <- 0.20 * nsamples(ps)
prevalenceThreshold

#Execute prevalence filter, using `prune_taxa()` function
keepTaxa <- rownames(prevdf)[(prevdf$Prevalence >= prevalenceThreshold)]
ps2 <- prune_taxa(keepTaxa, ps)
ps2
```

##Deseq2

```{r Running DESeq2, include=FALSE}
dds <- phyloseq_to_deseq2(ps, ~ Sample_Type)
colData(dds)

#First check for sparsity
#very sparse count datasets, with large counts for single samples per row and the rest at 0, don't fit well to the negative binomial distribution. 
#Here, the VST or simply shifted log, log(count+k), might be a safer choice than the rlog. 
#Test for sparsity by looking at a plot of the row sum of counts and the proportion of count which is in a single sample
#If too many points clustered up at 1 on y-axis there is too many zeros in the data set, consider filtering more
plotSparsity(dds, normalized=FALSE)

#Calculate geometric means prior to estimate size factors
#Use if DESeq fails due to issues with calculating geometric means, due to number of zeros in dataset. 
#Use with estimateDispersions and getVarianceStabilizedData to get stabilized data
#Remember that filtering appropriately can avoid this. 
gm_mean = function(x, na.rm=TRUE){
  exp(sum(log(x[x > 0]), na.rm=na.rm) / length(x))
}
geoMeans = apply(counts(dds), 1, gm_mean)
dds <- estimateSizeFactors(dds, geoMeans = geoMeans)

#DESeq function estimates size factors, dispersions and OTU-wise dispersion estimates.
#It then determines the relationship between mean disperison estimates and finalizes them. The data is fitted to the model and tested.
dds <- DESeq(dds, fitType="local", test = "Wald") #will takes 10-15 mins
plotDispEsts(dds)

#This function does the same as varianceStabilizingTransformation, but returns only transformation values. 
#Estimates steps in DESeq funtion individually, estimateSizeFactors, estimateDispersions & getVarianceStabilizedData
vst <- estimateDispersions(dds) #will take several mins
vst <- getVarianceStabilizedData(dds)
head(vst,3)
dim(vst) #Checking to make sure it has same dimentions as OTU table in phyloseq object
```

```{r}
#Making copy
ps_deseq <- ps
#Adding variance stabilized values to phyloseq object. These are the transformed values to be used for graphing above
otu_table(ps_deseq) <- otu_table(vst, taxa_are_rows = TRUE)

#This function calculates a variance stabilizing transformation (VST) from the fitted dispersion-mean relation(s) 
#and then transforms the count data (normalized by division by the size factors or normalization factors), 
#yielding a matrix of values which are now approximately homoskedastic (having constant variance along the range of mean values). 
#The transformation also normalizes with respect to library size. 
vsd <- varianceStabilizingTransformation(dds, blind=FALSE, fitType = "local")

#PCoA plot based on Transformed values
PCA <- plotPCA(vsd, intgroup = c("Sample_Type"), returnData=TRUE)
percentVar <- round(100 * attr(PCA, "percentVar"))
ggplot(PCA, aes(PC1, PC2, color=Sample_Type)) + geom_point(size=3) +
  xlab(paste0("PC1: ",percentVar[1],"% variance")) +
  ylab(paste0("PC2: ",percentVar[2],"% variance"))
```

##Looking into The Results of DESeq output
```{r}
#creating a table of the results of the tests
res_FvsGS <- results(dds, contrast=c("Sample_Type","Feces","Grab_Sample"), cooksCutoff = TRUE, independentFiltering=TRUE, alpha=0.05)
res_FvsS <- results(dds, contrast=c("Sample_Type","Feces", "Solid"), cooksCutoff = TRUE, independentFiltering=TRUE, alpha=0.05)
res_FvsST <- results(dds, contrast=c("Sample_Type","Feces","Stomach_Tube"), cooksCutoff = TRUE, independentFiltering=TRUE, alpha=0.05)
res_FvsLS <- results(dds, contrast=c("Sample_Type","Feces","Liquid_Strained"), cooksCutoff = TRUE, independentFiltering=TRUE, alpha=0.05)
res_FvsLU <- results(dds, contrast=c("Sample_Type","Feces","Liquid_Unstrained"), cooksCutoff = TRUE, independentFiltering=TRUE, alpha=0.05)

res_SvsST <- results(dds, contrast=c("Sample_Type","Solid","Stomach_Tube"), cooksCutoff = TRUE, independentFiltering=TRUE, alpha=0.05)
res_SvsGS <- results(dds, contrast=c("Sample_Type","Solid","Grab_Sample"), cooksCutoff = TRUE, independentFiltering=TRUE, alpha=0.05)
res_SvsLS <- results(dds, contrast=c("Sample_Type","Solid","Liquid_Strained"), cooksCutoff = TRUE, independentFiltering=TRUE, alpha=0.05)
res_SvsLU <- results(dds, contrast=c("Sample_Type","Solid","Liquid_Unstrained"), cooksCutoff = TRUE, independentFiltering=TRUE, alpha=0.05)

res_STvsGS <- results(dds, contrast=c("Sample_Type","Stomach_Tube","Grab_Sample"), cooksCutoff = TRUE, independentFiltering=TRUE, alpha=0.05)
res_STvsLS <- results(dds, contrast=c("Sample_Type","Stomach_Tube","Liquid_Strained"), cooksCutoff = TRUE, independentFiltering=TRUE, alpha=0.05)
res_STvsLU <- results(dds, contrast=c("Sample_Type","Stomach_Tube","Liquid_Unstrained"), cooksCutoff = TRUE, independentFiltering=TRUE, alpha=0.05)

res_GSvsLS <- results(dds, contrast=c("Sample_Type","Grab_Sample","Liquid_Strained"), cooksCutoff = TRUE, independentFiltering=TRUE, alpha=0.05)
res_GSvsLU <- results(dds, contrast=c("Sample_Type","Grab_Sample","Liquid_Unstrained"), cooksCutoff = TRUE, independentFiltering=TRUE, alpha=0.05)

res_LSvsLU <- results(dds, contrast=c("Sample_Type","Liquid_Strained","Liquid_Unstrained"), cooksCutoff = TRUE, independentFiltering=TRUE, alpha=0.05)

#Making list for lapply function
res <-list(res_FvsGS,res_FvsS,res_FvsST,res_FvsLS,res_FvsLU,res_SvsST,res_SvsGS,res_SvsLS,res_SvsLU,
           res_STvsGS,res_STvsLS,res_STvsLU,res_GSvsLS,res_GSvsLU,res_LSvsLU)
#sorting by lowest adjusted p-value
res<- lapply(res, function(res) res[order(res$padj, na.last=NA), ])
#looping to get summary of all results
summary <-lapply(res, function(res) summary(res))

#order by adjusted p-value and remove entries with NA
alpha <- 0.01
sigtab_FvsGS <- res[[1]][which(res[[1]]$padj < alpha), ]
sigtab_FvsS <- res[[2]][which(res[[2]]$padj < alpha), ]
sigtab_FvsST <- res[[3]][which(res[[3]]$padj < alpha), ]
sigtab_FvsLS <- res[[4]][which(res[[4]]$padj < alpha), ]
sigtab_FvsLU <- res[[5]][which(res[[5]]$padj < alpha), ]
sigtab_SvsST <- res[[6]][which(res[[6]]$padj < alpha), ]
sigtab_SvsGS <- res[[7]][which(res[[7]]$padj < alpha), ]
sigtab_SvsLS <- res[[8]][which(res[[8]]$padj < alpha), ]
sigtab_SvsLU <- res[[9]][which(res[[9]]$padj < alpha), ]
sigtab_STvsGS <- res[[10]][which(res[[10]]$padj < alpha), ]
sigtab_STvsLS <- res[[11]][which(res[[11]]$padj < alpha), ]
sigtab_STvsLU <- res[[12]][which(res[[12]]$padj < alpha), ]
sigtab_GSvsLS <- res[[13]][which(res[[13]]$padj < alpha), ]
sigtab_GSvsLU <- res[[14]][which(res[[14]]$padj < alpha), ]
sigtab_LSvsLU <- res[[15]][which(res[[15]]$padj < alpha), ]

#Making list for new lapply function
sigtab <-list(sigtab_FvsGS,sigtab_FvsS,sigtab_FvsST,sigtab_FvsLS,sigtab_FvsLU,sigtab_SvsST,sigtab_SvsGS,sigtab_SvsLS,
              sigtab_SvsLU,sigtab_STvsGS,sigtab_STvsLS,sigtab_STvsLU,sigtab_GSvsLS,sigtab_GSvsLU,sigtab_LSvsLU)
#Making dataframe of results and add taxonomic labels for plotting
sigtab <-lapply(sigtab, function(sigtab) cbind(as(sigtab, "data.frame"), as(tax_table(ps)[rownames(sigtab), ], "matrix")))

#Separating out just positive increases
posigtab <-lapply(sigtab, function(sigtab) sigtab[sigtab[, "log2FoldChange"] > 0, ])

#Adding column names to positive increased taxa
posigtab <-lapply(posigtab, function(posigtab) posigtab[, c("baseMean", "log2FoldChange", "lfcSE", "padj", "Phylum", "Class", "Family", "Genus", "Species")])

#subsetting Family for results plotted
sigtabFam <-lapply(sigtab, function(sigtab) subset(sigtab, !is.na(Family)))
```

```{r}
#Ordering by Phylum
PhylumOR <- lapply(sigtabFam, function(sigtabFam) (tapply(sigtabFam$log2FoldChange, sigtabFam$Phylum, function(PhylumOR) max(PhylumOR))))
PhylumOR <- lapply(PhylumOR, function(PhylumOR) sort(PhylumOR,TRUE))

#Phylum signifcantly different between groups
for (i in 1:15) {
sigtabFam[[i]]$Phylum <- factor(as.character(sigtabFam[[i]]$Phylum), levels=names(PhylumOR[[i]]))
}

#Ordering by Family
PhylumOR <- lapply(sigtabFam, function(sigtabFam) (tapply(sigtabFam$log2FoldChange, sigtabFam$Family, function(PhylumOR) max(PhylumOR))))
PhylumOR <- lapply(PhylumOR, function(PhylumOR) sort(PhylumOR,TRUE))

#Families signifcantly different between groups
for (i in 1:15) {
sigtabFam[[i]]$Family <- factor(as.character(sigtabFam[[i]]$Family), levels=names(PhylumOR[[i]]))
}

#Identifing OTU with lowest adjusted p.value
topOTU <- lapply(res, function(res) rownames(res)[which.min(res$padj)])

#Plot of OTU counts for OTU with lowest adjuested p.value (AKA most signficantly changed)
for (i in 1:15) {
plotCounts(dds, gene=topOTU[[i]], intgroup=c("Sample_Type"))
}

#Adding taxa names to data frame and add taxonomic labels for plotting
res_FvsGS.taxa <- cbind(as(res[[1]], "data.frame"), as(tax_table(ps)[rownames(res[[1]]), ], "matrix"))
res_FvsS.taxa <- cbind(as(res[[2]], "data.frame"), as(tax_table(ps)[rownames(res[[2]]), ], "matrix"))
res_FvsST.taxa <- cbind(as(res[[3]], "data.frame"), as(tax_table(ps)[rownames(res[[3]]), ], "matrix"))
res_FvsLS.taxa <- cbind(as(res[[4]], "data.frame"), as(tax_table(ps)[rownames(res[[4]]), ], "matrix"))
res_FvsLU.taxa <- cbind(as(res[[5]], "data.frame"), as(tax_table(ps)[rownames(res[[5]]), ], "matrix"))

res_SvsST.taxa <- cbind(as(res[[6]], "data.frame"), as(tax_table(ps)[rownames(res[[6]]), ], "matrix"))
res_SvsGS.taxa <- cbind(as(res[[7]], "data.frame"), as(tax_table(ps)[rownames(res[[7]]), ], "matrix"))
res_SvsLS.taxa <- cbind(as(res[[8]], "data.frame"), as(tax_table(ps)[rownames(res[[8]]), ], "matrix"))
res_SvsLU.taxa <- cbind(as(res[[9]], "data.frame"), as(tax_table(ps)[rownames(res[[9]]), ], "matrix"))

res_STvsGS.taxa <- cbind(as(res[[10]], "data.frame"), as(tax_table(ps)[rownames(res[[10]]), ], "matrix"))
res_STvsLS.taxa <- cbind(as(res[[11]], "data.frame"), as(tax_table(ps)[rownames(res[[11]]), ], "matrix"))
res_STvsLU.taxa <- cbind(as(res[[12]], "data.frame"), as(tax_table(ps)[rownames(res[[12]]), ], "matrix"))

res_GSvsLS.taxa <- cbind(as(res[[13]], "data.frame"), as(tax_table(ps)[rownames(res[[13]]), ], "matrix"))
res_GSvsLU.taxa <- cbind(as(res[[14]], "data.frame"), as(tax_table(ps)[rownames(res[[14]]), ], "matrix"))
res_LSvsLU.taxa <- cbind(as(res[[15]], "data.frame"), as(tax_table(ps)[rownames(res[[15]]), ], "matrix"))

#Determining taxa of most significantly changed OTU
#TopOTU_DLvsP <- res_HT_DLvsP.taxa[1,10:13]
#TopOTU_FSvsP <- res_HT_FSvsP.taxa[1,10:13]
#TopOTU_FSvsDL <- res_HT_FSvsDL.taxa[1,10:13]
#res_HT_DLvsP.taxa[grep("denovo2108", rownames(res_HT_DLvsP.taxa)), ]
```

##Corncob

We test all the taxa in our data to see if they are differentially-abundant or differentially-variable.vThe differentialTest function will these tests on all taxa, while controlling the false discovery rate to account for multiple comparisons.

```{r Running corncob, warning=FALSE}
#we do not include the response term because we are testing multiple taxa.

#We specify the covariates of our model using formula and phi.formula 
#We also specify which covariates we want to test for by removing them in the formula_null and phi.formula_null arguments.

# The difference between the formulas and the null version of the formulas
# will be the variables that are tested. In this case, as when we examined
# the single taxon, we will be testing the coefficients of Sample Type for
# both the expected relative abundance and the overdispersion.

# We set fdr_cutoff to be our controlled false discovery rate.
set.seed(1)
fullAnalysis <- differentialTest(formula = ~ Sample_Type, phi.formula = ~ Sample_Type,
                                 formula_null = ~ 1, phi.formula_null = ~ 1,data = ps, fdr_cutoff = 0.05)
```

```{r checking results of corncob}
#counting ASVs in each group
length(fullAnalysis$DA)
length(fullAnalysis$DV)

#getting taxonomy
DA_taxa <- otu_to_taxonomy(OTU=fullAnalysis$DA, data=ps)
DV_taxa <- otu_to_taxonomy(OTU=fullAnalysis$DV, data=ps)

#Getting p-values
fullAnalysis$p_fdr[1:10,]
```

Here we have identified 844 ASVs that are differentaily abundant and 397 ASVs that are differentaially variable.

We check into any taxa that we couldn't fit the model to.

```{r Looking for taxaa that don't fit model}
#checking if any taxa for which we were not able to fit a model
fullAnalysis$warning
otu_to_taxonomy(OTU ="Seq5706", data = ps)
```

The model not fitting to these taxa could be because the model is overparameterized (doubtfull since we only have one) or because there isn't enought observations. Or it maybe that the initializations were invaild for these taxa and it needs to be re-evaluated with new initializations.

Examining the these taxa
```{r}
otu_table(ps)[,"Seq5706"]
```

Next we run a model for these taxa that didn't fit the previous model

```{r model for taxa that didn't fit previous model}
check <- bbdml(formula=Seq5706 ~ Sample_Type, phi.formula = ~ Sample_Type, data=ps)
```

While the model fits, we should be skeptical of any statistical model fit on a single observed counts. 

```{r}
plot(check, color="Sample_Type")
```

#Alpha Diversity {.tabset}

##Richness

First we will look at richness without including our uncertainty of each measurement.
```{r}
#Exploring richness
## Let's look at the observed richness of the water samples
observed_c <- sample_richness(ps)
summary(observed_c)
plot(observed_c, ps, color = "SamplePhase")
```

Here we have a first look at the richness (number of observed ASVs), however, we will need to account for the differences in sequencing depth. To get an idea of the range of depth we will first grapth out the depth. 

```{r}
#Looking at samples at different depths as this may be confounded with observed richness. 
data.frame("observed_richness" = (observed_c %>% summary)$estimate,
           "depth" = phyloseq::sample_sums(ps), # Easter egg! Phyloseq's function to get depth
           "SamplePhase" = ps %>% sample_data %>% get_variable("SamplePhase"),
           "CowID" = ps %>% sample_data %>% get_variable("CowID")) %>%
  ggplot(aes(x = depth, y = observed_richness, color = SamplePhase)) +
  geom_point() + 
  geom_text(aes(label = CowID))
```

Here we see that sample the fecal sample from cow 2477 has 1,730,605 reads. Thus, there are big differences in depth. So, we will explore this a bit more. 

```{r}
#Get info on depth of sequecing for samples
data.frame("Min" = min(sample_sums(ps)),"Max" = max(sample_sums(ps)),
           "Range" = range(sample_sums(ps)), "median" = median(sample_sums(ps)))
#Looking to see what the majority depth is. 
data.frame("observed_richness" = (observed_c %>% summary)$estimate,
           "depth" = phyloseq::sample_sums(ps), # Easter egg! Phyloseq's function to get depth
           "Sample_Type" = ps %>% sample_data %>% get_variable("SamplePhase"),
           "CowID" = ps %>% sample_data %>% get_variable("CowID")) %>%
  ggplot(aes(x = depth, y = observed_richness, color = Sample_Type)) +
  geom_point() + ylim(0, 4000) + xlim(0,15000)
```

Looks like the majority of the samples have under 15,000 reads. 13 Samples have more reads than 15,000. The soild and feces in general have lower sequencing depth and thus lower observed rightness.

```{r}
#Get info on depth of sequecing for samples
data.frame("Min" = min(sample_sums(ps)),"Max" = max(sample_sums(ps)),
           "Range" = range(sample_sums(ps)), "median" = median(sample_sums(ps)))
```

The sequencing depth ranges from 1,884 to 1.73 million reads.

###Running alpha diversity estimate

```{r}
#This is an alpha diversity estimate -- a special class for alpha diversity estimates
ba <- breakaway(ps)
ba 
#plotting 
plot(ba, ps, color = "Sample_Type")
```

The error bars here are quite large, but this is to be expected as there is a lot of uncertainty in estimating alpha diversity.

```{r}
#testing the hypothesis that different sample types have the same microbial diversity
#betta() works like a regression model but it accounts for the uncertainty in estimating diversity
#Testing differences between Sample Phases
bt_ST <- betta(summary(ba)$estimate,
            summary(ba)$error,
            make_design_matrix(ps, "Sample_Type"))
bt_ST$table
#Testing differences between Rumen and feces samples
bt_SL <- betta(summary(ba)$estimate,
            summary(ba)$error,
            make_design_matrix(ps, "SampleLocation"))
bt_SL$table

#Note that these estimates account for different sequencing depths!
#breakaway estimates the number of missing species based on the sequence depth and number of rare taxa in the data
```

When comparing rumen to fecal samples betta() estimates that the mean species-level diversity in feces is 2890 "species". It estimates that the diversity in the rumen is not significantly different. When you break the rumen samples up into different sample types betta() estimates the mean species-level diversity in feces is 2738 "species". Liquid strained and unstrained samples have significantly higher "species" (on average 2035 and 2427 species respectively). However, solid samples from the rumen have a lower number of "spieces", but it wasn't significant from fecal samples.

##Evenness

No taxa are found in all samples so first we examine these data to pick a good base taxa that reduces the amount of variability (Divnet need this to run). We will do this by ooking for an ASV that is found in the most samples, this might be the taxa that is the most variance stabilizing.

```{r warning=FALSE}
#https://github.com/adw96/DivNet/issues/14
#The best I can do is to use an ASV that is found in 98% of samples
Core <- filter_taxa(ps, function(x) sum(x >= 1) > (0.98*length(x)), TRUE)
tax_table(Core) #Seq10878
```


```{r}
#Running DivNet
dv_ps <- divnet(ps, ncores = 3, base="Seq10878")  #This is 11.8GB
#DivNet outputs a list of the estimates shannon, simpson (alpha diversity) bray-curtis, euclidean (beta diversity)
dv_ps %>% names

#You can pull them out individually:
dv_water$shannon %>%
  summary %>%
  add_column("Sample_type" = ps %>% otu_table %>% sample_names)

# or plot them:
plot(c$shannon, ps, col = "SamplePhase")
```
